# AI Lens Helper - GUI 사용 가이드

YOLO+CLIP 기반 전시품 인식 시스템 GUI 애플리케이션

## 실행 방법

```bash
python3 gui_app.py
```

## 기능

### 📚 Tab 1: 인덱스 빌드 (학습)

수집한 이미지 데이터로 YOLO+CLIP 인덱스를 생성합니다.

#### 설정 항목

1. **데이터 루트**: 수집한 이미지 데이터가 있는 디렉토리
   - 기본값: `./data`
   - 구조: `data/{place}/{item}/*.jpg`

2. **장소 (Place)**: 인덱스를 만들 장소 이름
   - 예: "경복궁", "국립중앙박물관"

3. **저장 경로**: 생성될 인덱스 파일 경로
   - 예: `./indexes/경복궁_clip`
   - 자동으로 `.json`과 `.faiss` 파일 생성

4. **Device**: 실행 디바이스
   - `cpu`: CPU에서 실행 (느리지만 호환성 높음)
   - `cuda`: GPU에서 실행 (빠름, NVIDIA GPU 필요)

5. **YOLO 모델**: 사용할 YOLO 모델 크기
   - `yolov8n.pt`: Nano (가장 빠름, 가벼움)
   - `yolov8s.pt`: Small
   - `yolov8m.pt`: Medium
   - `yolov8l.pt`: Large (가장 정확하지만 느림)

#### 사용 방법

1. 설정 값을 입력/선택
2. "🚀 인덱스 빌드 시작" 버튼 클릭
3. 로그 창에서 진행 상황 확인
4. 완료되면 팝업 메시지 확인

#### 출력

- `{저장경로}.json`: 메타데이터 및 임베딩 정보
- `{저장경로}.faiss`: FAISS 검색 인덱스

---

### 🔍 Tab 2: Inference (테스트)

학습된 인덱스로 실제 이미지를 테스트합니다.

#### 사용 방법

**1단계: 모델 로드**
1. "인덱스 파일" 경로 입력 또는 "찾아보기"로 선택
   - 예: `./indexes/경복궁_clip.json`
2. "모델 로드" 버튼 클릭
3. 상태가 "✓ 모델 로드 완료"로 변경되면 성공

**2단계: 이미지 선택**
1. "📷 이미지 선택" 버튼 클릭
2. 테스트할 이미지 파일 선택
3. 선택한 이미지가 오른쪽 캔버스에 표시됨

**3단계: 설정**
1. **Place**: 이미지가 속한 장소 (모델과 일치해야 함)
2. **Top-K**: 상위 몇 개 예측을 표시할지 (1~10)
3. **Threshold**: Accept/Recollect 판단 기준 (0.0~1.0)
   - 높을수록 엄격 (0.7 권장)

**4단계: Inference 실행**
1. "🔍 Inference 실행" 버튼 클릭
2. 결과가 하단 텍스트 영역에 표시됨
3. Accept/Recollect 판정 팝업 확인

#### 결과 해석

**Accept (수락)**
```
📋 결정: ACCEPT
🎯 선택된 전시품: 근정전
📊 신뢰도: 0.8542
```
- 신뢰도가 threshold 이상
- 전시품이 올바르게 식별됨
- 선택된 전시품 정보 표시

**Recollect (재촬영)**
```
📋 결정: RECOLLECT
🎯 선택된 전시품: 근정전
📊 신뢰도: 0.6234
💡 개선 힌트:
  • Ensure the exhibit fills the frame.
  • Reduce reflections and glare.
  • Capture with a steady hand for sharpness.
```
- 신뢰도가 threshold 미만
- 사진을 다시 찍어야 함
- 개선 힌트 제공

**Top-K 예측**
```
🏆 Top-5 예측:
  ✓ 1. 근정전: 0.8542
    2. 경회루: 0.7821
    3. 광화문: 0.7234
    4. 사정전: 0.6891
    5. 교태전: 0.6543
```
- 유사도 순으로 정렬
- 1위가 최종 선택 결과

---

## 사용 예시

### 시나리오 1: 경복궁 전시품 인덱스 생성

1. Tab 1 (인덱스 빌드) 선택
2. 설정:
   - 데이터 루트: `./data`
   - 장소: `경복궁`
   - 저장 경로: `./indexes/경복궁_clip`
   - Device: `cpu`
   - YOLO 모델: `yolov8n.pt`
3. "인덱스 빌드 시작" 클릭
4. 완료 대기 (수십 분 소요 가능)

### 시나리오 2: 근정전 사진 테스트

1. Tab 2 (Inference) 선택
2. 모델 로드:
   - 인덱스 파일: `./indexes/경복궁_clip.json`
   - "모델 로드" 클릭
3. 이미지 선택:
   - "이미지 선택" 클릭
   - 근정전 사진 선택
4. 설정:
   - Place: `경복궁`
   - Top-K: `5`
   - Threshold: `0.7`
5. "Inference 실행" 클릭
6. 결과 확인

---

## 문제 해결

### "모델을 찾을 수 없습니다"
- 인덱스 파일 경로가 올바른지 확인
- `.json` 파일과 `.faiss` 파일이 같은 디렉토리에 있는지 확인

### "Place mismatch" 오류
- Inference 설정의 Place가 인덱스 빌드 시 사용한 Place와 일치하는지 확인

### 인덱스 빌드가 너무 느림
- Device를 `cuda`로 변경 (GPU 필요)
- 더 작은 YOLO 모델 사용 (`yolov8n.pt`)
- 이미지 개수 줄이기

### Inference 결과가 부정확함
- Threshold를 낮춰보기 (0.6 ~ 0.65)
- 더 많은 학습 이미지로 인덱스 재빌드
- 더 큰 YOLO 모델 사용 (`yolov8m.pt` 이상)

---

## 기술 스택

- **GUI**: Tkinter (Python 기본 GUI 라이브러리)
- **YOLO**: YOLOv8 (객체 검출)
- **CLIP**: OpenCLIP ViT-B/16 (이미지 임베딩)
- **검색**: FAISS (고속 nearest neighbor search)

---

## 팁

### 좋은 학습 데이터 만들기

1. **다양한 각도**: 같은 전시품을 여러 각도에서 촬영
2. **다양한 조명**: 밝기가 다른 환경에서 촬영
3. **적절한 거리**: 전시품이 프레임의 50~80% 차지하도록
4. **최소 이미지 수**: 전시품당 최소 10장 이상 권장

### 좋은 테스트 사진 찍기

1. **전시품 중심**: 전시품이 화면 중앙에 오도록
2. **흔들림 방지**: 손떨림 최소화
3. **반사 최소화**: 유리 반사 피하기
4. **적절한 거리**: 너무 가깝거나 멀지 않게

---

## 라이선스

이 프로젝트는 ai-lens-helper의 일부입니다.
